# 📝 Fine-Tuning GPT-2 on a Custom Dataset

This project demonstrates how to fine-tune **GPT-2** using a **custom dataset** with Hugging Face’s `transformers` library on **Google Colab** (GPU-enabled).  
The trained model can generate coherent and creative text that mimics the style and tone of the dataset it was trained on.

---

## 🚀 Features
- **Custom Dataset Training** – Fine-tuned GPT-2 on your own text data.
- **Creative Text Generation** – Produces context-aware, stylistically aligned outputs.
- **GPU Acceleration** – Trained efficiently using Google Colab’s GPU runtime.
- **Hugging Face Integration** – Uses the `transformers` and `tokenizers` libraries for a smooth workflow.
- **Experiment Tracking** – Integrated with **Weights & Biases** for monitoring.

---

## 🛠️ Tools & Libraries
- [Transformers](https://github.com/huggingface/transformers)
- [PyTorch](https://pytorch.org/)
- [Hugging Face Tokenizers](https://github.com/huggingface/tokenizers)
- [Google Colab](https://colab.research.google.com/)
- [Weights & Biases](https://wandb.ai/)
